1) 「1.3 マッチング機能」を最短で動かす設計と実装
A. 全体像（2ステップ + お好みで強化）
1. ハード条件フィルタ（例: 就業場所/ビザ必須/最低年収/就業形態）
2. スコアリング
    * ルール重みづけ（経験年数のギャップ、必須スキル充足率 等）
    * ＋ セマンティック類似度（埋め込みのコサイン類似度）
    * 必要なら ハイブリッド検索（BM25＋ベクトル） で精度底上げ
    * 最後に 再ランク（Cross-Encoder/LTR） で上位だけ精緻化
コサイン類似度は「正規化した内積」です（式の定義はscikit-learn公式を参照）。scikit-learn.org+1 ハイブリッド検索はBM25とベクトル検索を並行実行し、正規化スコアを融合（RRFなど）して順位を作るのが定石です。docs.weaviate.io+2docs.weaviate.io+2
B. データ設計（PostgreSQL）
* jobs（求人）/candidates（候補者）/skills（正規化テーブル）/candidate_skills/job_requirements
* 将来のAI強化に備え、テキスト要約と正規化スキル列を用意（例: candidate.profile_text, job.requirements_text）
* ベクトルは外部（Pinecone/Weaviate）か、pgvectorで内製運用（小～中規模ならpgvectorも現実的）
pgvector代替（Postgres内で完結したい派向け）

-- 拡張
CREATE EXTENSION IF NOT EXISTS vector;

-- OpenAI text-embedding-3-large(最大3072次元)に合わせた列（※実際の次元は使うモデルとdimensionsに合わせる）
ALTER TABLE candidates ADD COLUMN embedding vector(3072);
ALTER TABLE jobs       ADD COLUMN embedding vector(3072);

-- 近似探索のためのHNSWインデックス（コサイン距離）
CREATE INDEX ON candidates USING hnsw (embedding vector_cosine_ops);
CREATE INDEX ON jobs       USING hnsw (embedding vector_cosine_ops);
pgvectorはHNSW/IVFFlatの両方式に対応。HNSWは検索精度-速度のトレードオフで有利、IVFFlatは構築が速く軽量。運用の勘所も公開情報が多いです。GitHub+2Microsoft Learn+2
C. スコアリング（まずは“動く”シンプル版）

# ルール + 類似度の重みづけ例
def rule_score(candidate, job):
    # 必須: 地域/雇用形態/ビザ等 … 不一致なら-∞（除外）
    if candidate.location != job.location and not job.remote_ok:
        return None  # ハードフィルタで脱落
    if not candidate.has_all(job.must_have_skills):
        return None

    # ソフト: 年収ギャップ, 経験年数差, Nice-to-haveスキル充足率 等を0-1に正規化
    s1 = candidate.skill_coverage(job)          # 0..1
    s2 = 1 - abs(candidate.years - job.req_years) / 10.0
    s3 = candidate.salary_expectation_fit(job)  # 0..1

    return 0.5*s1 + 0.3*s2 + 0.2*s3

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def blended_score(rule, cand_vec, job_vec, alpha=0.6):
    if rule is None:
        return None
    # コサイン類似度（-1..1）→ 0..1 に線形変換
    cos = float(cosine_similarity(cand_vec.reshape(1,-1), job_vec.reshape(1,-1))[0,0])
    cos01 = (cos + 1) / 2.0
    return alpha*cos01 + (1-alpha)*rule
cosine_similarityの定義はscikit-learnに明記されています。scikit-learn.org
D. ハイブリッド検索（精度をもう一段上げる）
* BM25（Elasticsearch/OpenSearch）で「必須ワード」「ドメイン語彙」を拾い、
* ベクトル近傍で文脈類似を拾う
* RRFで両方の上位K件を融合 → その上位NだけCross-Encoderで再ランク ElasticsearchやWeaviateはRRFやハイブリッドを公式にサポートしています。Elastic+2Elastic+2

2) 「フェーズ4: AI・機械学習システム」の作り方（実コード付き）
2.1 埋め込み（Embeddings）生成
* OpenAI text-embedding-3-large（最大3072次元）または-3-small（軽量）。dimensionsパラメータで縮約可能＝コスト/精度を調整できます。openai.com
* APIの呼び出し方はEmbeddings APIリファレンス参照。platform.openai.com

# pip install openai
from openai import OpenAI
client = OpenAI()  # 環境変数 OPENAI_API_KEY を使用

def embed_text(text: str, dims: int = 1024):
    # 例: text-embedding-3-large を 1024次元に短縮
    emb = client.embeddings.create(
        model="text-embedding-3-large",
        input=text,
        dimensions=dims
    ).data[0].embedding
    return emb
2.2 ベクターデータベース統合（Pinecone例）
* **Upsert（登録）→ Query（近傍検索）**の2操作を押さえればOK。docs.pinecone.io+1

# pip install "pinecone-client[grpc]"
from pinecone.grpc import PineconeGRPC as Pinecone
pc = Pinecone(api_key="YOUR_PINECONE_API_KEY")
index = pc.Index("match-engine")

def upsert_candidate_vector(cid: str, text: str):
    vec = embed_text(text)  # 例: 1024次元
    index.upsert(vectors=[{"id": cid, "values": vec, "metadata": {"type":"candidate"}}])

def query_job_to_candidates(job_text: str, top_k=50):
    qvec = embed_text(job_text)
    return index.query(vector=qvec, top_k=top_k, include_metadata=True)
PineconeのUpsert・Queryの基本は公式ドキュメントが最も早いです。docs.pinecone.io+1 Pineconeは埋め込み統合インデックスも提供（テキスト投入だけで自動ベクトル化）—要件とコストで選択。docs.pinecone.io
Weaviate派なら、BM25＋ベクトルのハイブリッド検索が素直に書けます。docs.weaviate.io
pgvector派ならPostgres内で一気通貫。HNSW/IVFFlatの選択やパラメタ調整の考え方は公知です。GitHub+1
2.3 再ランク（Rerank：Cross-Encoder / LTR）
「上位50～200件」をCross-EncoderやLearning-to-Rankで再並べ替えすると精度がグッと上がります。
* Cross-Encoderの代表: cross-encoder/ms-marco-MiniLM-L6-v2 / L12-v2（HuggingFace）huggingface.co+1
* LTRの概念と二段階パイプラインはElastic公式が分かりやすい。Elastic

# pip install sentence-transformers
from sentence_transformers import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")
def rerank(query_text, passages: list[str], top_k=20):
    pairs = [(query_text, p) for p in passages]
    scores = reranker.predict(pairs)   # 高いほど関連
    ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)
    return ranked[:top_k]
2.4 マッチングスコアの“説明可能性”（Explainability）
* Tabular系の再ランクや合格率予測にはSHAPで「なぜその順位・確率か」を示すのが定番。SHAP+1

# 例: XGBoostで合格確率を学習し、SHAPで要因を可視化
# pip install xgboost shap
import xgboost as xgb
import shap, numpy as np

X_train, y_train = ...  # 特徴: スキル一致度/経験差/面接回数など
model = xgb.XGBClassifier(eval_metric="logloss")
model.fit(X_train, y_train)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_train[:100])
# shap.summary_plot(shap_values, X_train[:100])  # Notebook等で可視化
XGBoostとそのパラメータ、SHAPの解説は公式が充実。XGBoost Documentation+1
2.5 予測タスク（例：面接合格率）
* 特徴量: スキル一致度、職務経歴ギャップ、面接回数、企業側評価、年収乖離、レコメンド経路 等
* モデル: ロジスティック回帰（解釈しやすい）→ XGBoost（非線形で高精度）
* 評価: AUC/PR、キャリブレーション（信頼度の整合性）、NDCG@K（ランキング品質）も併用。scikit-learn.org
2.6 精度を“本番で落とさない”評価設計
* オフライン: NDCG@K / DCG, Precision@K を履歴データで検証。scikit-learn.org+1
* オンライン: ABテスト、スコアの分布監視、データドリフト検知
* ログ: 候補者・求人・スコア・クリック・成約を全て時系列で保存（後学習/LTR特徴に効く）
2.7 MLOps（最低限）
* モデル・実験管理: MLflow Model Registry（バージョン/ステージ管理）mlflow.org+1
* データ品質: Great Expectationsでバリデーション→Data Docs自動生成docs.greatexpectations.io+1
* パイプライン: バッチ（再計算）＋ ストリーム（新着更新）/ 監視・アラート

3) API & スクリーンに落とす（最小実装）
3.1 FastAPI（サーバ）骨子

# app.py
from fastapi import FastAPI, Query
from pydantic import BaseModel
from typing import List

app = FastAPI()

class MatchItem(BaseModel):
    id: str
    score: float
    reason: List[str]  # 説明可能性の簡易メモ

@app.get("/match/jobs/{job_id}", response_model=List[MatchItem])
def match_candidates(job_id: str, top_k: int = 50):
    # 1) 取得: jobテキスト
    job_text = load_job(job_id).requirements_text
    # 2) 近傍: ベクトルDBで上位候補者
    hits = query_job_to_candidates(job_text, top_k=200)
    # 3) 再ランク: Cross-Encoder
    passages = [format_candidate(h) for h in hits]
    reranked = rerank(job_text, passages, top_k=top_k)
    # 4) ルールと混合スコア、理由を付与
    out = []
    for passage, rrscore in reranked:
        cand = parse_passage(passage)
        rscore = rule_score(cand, load_job(job_id))
        blend = blended_score(rscore, cand.vec, embed_text(job_text))
        reason = build_reason(cand, job_id, rscore, rrscore)
        out.append(MatchItem(id=cand.id, score=float(blend), reason=reason))
    return out
3.2 画面（管理UI）
* マッチ一覧: スコア降順 / フィルタ（勤務地, 年収, スキル, 最終更新）
* 理由表示: 「必須5/6スキル一致」「年収レンジ△50万」「類似度0.83」などバッジ化
* 調整: 重みスライダー（スキル比重/経験年数/類似度）→即時再計算（Redisキャッシュ）

4) ハイブリッド検索を使う場合の実装ヒント
Elasticsearch（BM25）× ベクトル（Pinecone/Weaviate）
1. BM25で上位Kを取得（求人→候補者 or 候補者→求人）
2. ベクトルで上位Kを取得
3. RRFで順位融合（1/(k + rank)の和）し上位Nを抜き出す
4. Cross-Encoderで再ランク
BM25はElasticでデフォルトのランキング。チューニングの考え方やk1,bの注意点はElastic公式が詳しいです。Elastic+2Elastic+2 RRFも公式ドキュメント化されています。Elastic Weaviateはハイブリッドの仕組みと融合の流れを丁寧に解説しています。docs.weaviate.io

5) パフォーマンス&運用の実践Tips
* 応答5秒以内：
    * 取り敢えず**ANNインデックス（HNSW/IVF）**を使う
    * 候補プールを200件以内→再ランクは50件以内に絞る
    * Redisキャッシュ（同一求人の再検索）
* スケール：
    * Pinecone/Weaviateのマルチテナント分離（namespaces等）を活用。ステータス監視やベクトル数はAPIで把握可。docs.pinecone.io
* 一貫性：
    * 埋め込みは同じモデル/次元で統一（OpenAIのdimensions縮約機能を使うなら、再計算も同条件で）。openai.com
* 説明責任：
    * スコア内訳（ルール/類似度/再ランク）をログ化→UI表示
* 評価：
    * オフラインでNDCG@K、オンラインでABテスト。scikit-learnのndcg_scoreは手軽。scikit-learn.org

6) スモールスタート → 本格AIの段階的ロードマップ
1. Week 1–2: ハード条件＋ルール重み（SQL/ Pythonのみ）
2. Week 3–4: 埋め込み＋ベクトル近傍（Pinecone/pgvector）
3. Week 5–6: ハイブリッド（BM25＋ベクトル）
4. Week 7–8: 再ランク（Cross-Encoder/LTR）
5. 継続: 合格率予測（XGBoost）＋SHAPで説明可能性

付録：実装断片（“そのまま貼って動かす”用）
A) 候補者・求人テキストの標準化例

def canonicalize_candidate(c):
    parts = [
        f"Title: {c.title}",
        f"Years: {c.years}",
        f"Skills: {', '.join(sorted(set(c.skills)))}",
        f"Summary: {c.summary}",
        f"Location: {c.location}",
        f"SalaryExpect: {c.salary_expect}"
    ]
    return "\n".join(parts)

def canonicalize_job(j):
    parts = [
        f"Role: {j.title}",
        f"ReqYears: {j.req_years}",
        f"Must: {', '.join(j.must_skills)}",
        f"Nice: {', '.join(j.nice_skills)}",
        f"Desc: {j.description}",
        f"Location: {j.location}",
        f"SalaryRange: {j.salary_range}"
    ]
    return "\n".join(parts)
B) Pineconeへ一括Upsert

def bulk_upsert_candidates(candidates):
    vecs = []
    for c in candidates:
        text = canonicalize_candidate(c)
        vec = embed_text(text)  # 例: 1024次元
        vecs.append({"id": c.id, "values": vec, "metadata": {"type":"candidate"}})
    index.upsert(vectors=vecs)
C) RRFによる融合の簡易実装

def rrf_fuse(bm25_ids, vector_ids, k=60):
    # 入力: それぞれ上位リスト（IDの配列）
    ranks = {}
    for r, id_ in enumerate(bm25_ids, start=1):
        ranks[id_] = ranks.get(id_, 0) + 1/(k + r)
    for r, id_ in enumerate(vector_ids, start=1):
        ranks[id_] = ranks.get(id_, 0) + 1/(k + r)
    return [id_ for id,_ in sorted(ranks.items(), key=lambda x: x[1], reverse=True)]

まとめ（要点だけ掴みたい人向け）
* まずは「ハード条件 → ルール重み → ベクトル類似」の二段＋αで“使える”マッチングにする。
* 次に BM25とベクトルのハイブリッド + RRF → Cross-Encoder再ランクで精度を底上げ。docs.weaviate.io+1
* 埋め込みはOpenAI text-embedding-3-*（dimensionsで縮約可）。Pinecone/Weaviate/pgvectorはどれを選んでも王道構成。openai.com+1
* 評価はNDCG@Kなどのランキング指標とABテスト。説明可能性はSHAPで。scikit-learn.org+1

参考（一次情報）
* OpenAI Embeddings（text-embedding-3-*と3072次元/縮約の説明）: https://openai.com/index/new-embedding-models-and-api-updates/ openai.com
* Embeddings APIリファレンス: https://platform.openai.com/docs/api-reference/embeddings platform.openai.com
* Pinecone（Upsert/Query/Indexの基礎）: https://docs.pinecone.io/guides/data/upsert-data, https://docs.pinecone.io/reference/api/2024-07/data-plane/query, https://docs.pinecone.io/guides/indexes/understanding-indexes docs.pinecone.io+2docs.pinecone.io+2
* ハイブリッド検索（BM25＋ベクトル＋RRF）: Weaviate Docs/Blog, Elastic Docs（RRF,LTR,BM25） Elastic+3docs.weaviate.io+3Weaviate+3
* コサイン類似度（scikit-learn）: https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html scikit-learn.org+1
* NDCG（scikit-learn）: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html scikit-learn.org
* XGBoost: https://xgboost.readthedocs.io/ XGBoost Documentation
* SHAP: https://shap.readthedocs.io/ SHAP

最後に： 「まず“動く”→“当たる”→“説明できる”」の順で厚くしていけば、要件（精度80%・5秒以内・500同時）に十分到達できます。もし社内の既存DBが強いならpgvector、スケーリング優先ならPinecone/Weaviate。迷ったらPinecone + OpenAI + Elasticの三点セットから入るのが、開発速度と運用の落としどころ的に無難です。
